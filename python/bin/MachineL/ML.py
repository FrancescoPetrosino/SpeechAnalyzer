# -*- coding: utf-8 -*-
"""
Created on Thu Jul 16 13:51:24 2020

@author: Aldo
"""


import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
#import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from pathlib import Path

import joblib

documents = []
path = Path("python/bin/files/")
for x in path.iterdir():
    data = ""
    with open(x, encoding="utf8", errors='ignore') as myfile:
       data = myfile.read()
       #print(data)
    documents.append(data)



stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')

punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)

def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)
X3 = vectorizer3.fit_transform(documents)
words = vectorizer3.get_feature_names()
print(len(words))



kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 3) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)
kmeans.fit(X3)
# We look at 3 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:,-1:-35:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))



filename = 'python/bin/MachineL/k_model.sav'
joblib.dump(kmeans, filename)

joblib.dump(vectorizer3, 'python/bin/MachineL/vectroizer.pkl')

Y = vectorizer3.transform(["today my team has scored 4 goals"])
prediction = kmeans.predict(Y)
print(prediction)