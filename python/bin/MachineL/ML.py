# -*- coding: utf-8 -*-
"""
Created on Thu Jul 16 13:51:24 2020

@author: Aldo
"""


#used to store model , future implementation

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
#import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from pathlib import Path
import os
import pickle

stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')


def tokenizeText(text):
        return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

def init():
    documents = []
    path = Path("python/bin/files/")
    for x in path.iterdir():
        data = ""
        with open(x, encoding="utf8", errors='ignore') as myfile:
            data = myfile.read()
        #print(data)

        documents.append(data)
   
    punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
    stop_words = text.ENGLISH_STOP_WORDS.union(punc)
    
    vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenizeText, max_features = 1000)
    X3 = vectorizer3.fit_transform(documents)
    words = vectorizer3.get_feature_names()
    print(len(words))
    kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 3) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)
    kmeans.fit(X3)
    # We look at 3 the clusters generated by k-means.
    common_words = kmeans.cluster_centers_.argsort()[:,-1:-35:-1]
    for num, centroid in enumerate(common_words):
        print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))

    return [kmeans,vectorizer3]

if __name__ == "__main__":
    list = init()
    #joblib.dump(list[0], 'python/bin/MachineL/k_model.sav')
    #joblib.dump(list[1], 'python/bin/MachineL/vectroizer.pkl')
    Pkl_Filename = "Model.pkl"
    with open(os.path.join('./python/bin/MachineL',Pkl_Filename), 'wb') as file:  
        pickle.dump(list[0], file)

    Pkl_Filename2 = "vect.pkl"
    with open(os.path.join('./python/bin/MachineL',Pkl_Filename2), 'wb') as file2:  
        pickle.dump(list[1], file2)


'''
Y = vectorizer3.transform(["today my team has scored 4 goals"])
prediction = kmeans.predict(Y)
print(prediction)
'''