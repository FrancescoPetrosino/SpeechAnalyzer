{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SpeechAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Progetto Technologies for Advanced Programming 2019/2020\n",
    "\n",
    "- Aldo Fiorito X81000447\n",
    "- Francesco Petrosino X81000533  \n",
    "Studenti presso Università di Catania cdl Informatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea e scopo di SpeechAnalyzer\n",
    "L'idea è nata osservando una community di persone all'interno dell'applicativo VOIP per le chat vocali Discord. \n",
    "<br \\>\n",
    "La domanda principale posta è stata \"Chi è il più chiacchierone del gruppo?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bisogna fare prima una premessa ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](train.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Da qui SpeechAnalyzer\n",
    "\n",
    "Fornisce statistiche e metriche su numerosi flussi vocali all'interno della propria rete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architettura dell'applicativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In breve\n",
    "1. L'utente si registra all'applicativo\n",
    "2. Un file json costruito dal suo flusso vocale viene mandato a un kafka-producer, avente come topic un campo   specificato dall'user\n",
    "3. Spark , essendo anche kafka-consumer , processa ed elabora altre informazioni dal dato e costruisce un datafraframe attraverso sparksql \n",
    "4. La nuova struttura creata viene indicizzata su elasticsearch\n",
    "5. Kibana si occupa di ottenere le metriche e le statistiche dai dati indicizzati attraverso un'interfaccia user-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flusso operativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'utente,tramite una form, viene fatto registrare all'applicativo inserendo il proprio nome,la propria compagnia( che sarà il topic kafka) e la propria lingua d'appartenenza.  \n",
    "**Perchè la lingua?**\n",
    "Perchè l'analizzatore riesce ad elaborare qualiasi lingua ( al momento le 5 principali ).  \n",
    "Ogni input verrà tradotto nella stessa lingua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PreProcessing del dato\n",
    "Un primo step di preprocessing dell'input vocale è quello di filtraggio del testo.       \\\n",
    "Attraverso un file in locale di \"badwords\" verranno sostituite quelle parole considerate blasfeme o offensive\n",
    "Come secondo step il testo filtrato viene tradotto in inglese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kafka\n",
    "E' la prima tecnologia della pipeline che andremo ad utilizzare\n",
    "( spiegare kafka e zookeaper )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Il topic-kafka verrà creato da codice python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "admin_client = KafkaAdminClient(\n",
    "                   bootstrap_servers=\"192.168.1.28:9092\", \n",
    "                   client_id='test'\n",
    "               ) <br />\n",
    "topic_list = [NewTopic(name=topicKafka, num_partitions=1, replication_factor=1)]\n",
    "admin_client.create_topics(new_topics=topic_list, validate_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il file json inviato al kafka-producer avrà questa struttura :\\\\ \n",
    "\n",
    "{\"name\":\"saro\" , \"message\" : \"Microsoft has released security updates \",\"topic\" : \"Tech\",\"language\" : \"en\",\"id\":\"75df85bf\"}\n",
    "\n",
    "Dove topic si riferisce al predict ottenuto dall'algoritmo di kmeans implementato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modello Machine Learning e K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'immagine appena mostrata rappresenta il modello di kmeans utilizzato e salvato in locale attraverso la libreria \"pickle\" <br />\n",
    "Per inizializzare il modello è stato dato in pasto un dataset di news ottenuto al seguente indirizzo URL\n",
    "http://mlg.ucd.ie/datasets/bbc.html. <br />\n",
    "Le news appartengono a 5 categorie diverse: business, entertainment, politics, sport, tech.\n",
    "Questi documenti rawText devono essere processati prima di essere passati all'algoritmo.\n",
    "StopWords,rimuovere le parole insignificanti\n",
    "Stemming,portare le parole alla root form\n",
    "tokenization , per"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Salvataggio e restore del modello ML\n",
    "k_model=MLAPI()\n",
    "Pkl_Filename = \"kmodel.pkl\"\n",
    "    with open(os.path.join('./python/bin/MachineL',Pkl_Filename), 'wb') as file:  \n",
    "        pickle.dump(k_model,file)\n",
    "        \n",
    "with open('./python/bin/MachineL/kmodel.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)      \n",
    "\n",
    "Questo permette una rapida esecuzione dell'algoritmo, in quanto l'oggetto viene semplicemente caricato all'avvio e non inizializzato di nuovo.\n",
    "Il predict del topic del kmeans viene effettuato solamente attraverso una semplice chiamata a metodo \\\n",
    "number = model.getPrediction(translated.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kafka Producer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['192.168.1.28:9092'],\n",
    "                         value_serializer=lambda x: \n",
    "                         dumps(x).encode('utf-8')\n",
    "                        )    <br />\n",
    "producer.send(topicKafka,value=data)   <br />\n",
    "\n",
    "L'indirizzo ip del boostrap_servers l'indirizzo ip statico settato a inizio per kafkaServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark \n",
    "Spiegazione spark\n",
    "\n",
    "\n",
    "Avrà anche il compito di essere un kafka consumer <br \\>\n",
    "E' stato scelto di usare il classico approccio **Receiver-based Approach** in quanto l'approccio diretto ( essendo scritto in Python , non è disponibile)\n",
    "\n",
    "KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1},)\n",
    "\n",
    "Dove ssc è lo streaming context di spark con intervallo microbatch di 2 secondi\n",
    "zkQuorum ,indirizzo più porta dove gira zookeaper , nel nostro caso \"127.0.0.1:2181\"\n",
    "Un nome per il consumer group id\n",
    "Quanti topic per kafkapartitions leggere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ricezione e preparazione della struttura\n",
    "\n",
    "Viene definito uno schema per accogliere il json :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"id\",StringType(),True),StructField(\"name\", StringType(), True),StructField(\"message\", StringType(), True),StructField(\"topic\", StringType(),True),StructField(\"language\",StringType(),True)])\n",
    "storage = sqlContext.createDataFrame(sc.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Successivamente si è passati alle funzioni lambda per estrapolare i dati al fin di calcolare metriche e statistiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](lambda.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il tutto viene inserito in un dataframe a cui,a fine riempimento,viene applicata una funzione lambda\n",
    "\n",
    "appendend.rdd.map(lambda item: {'timestamp': milli ,'id': item['id'],'name': item['name'],'message': item['message'],'profanity_count': item['profanity_count'],'words_count': item['word_count'],'language': item['language'],'topic': item['topic']})\n",
    "\n",
    "final_rdd = new.map(json.dumps).map(lambda x: ('key', x))\n",
    "<br \\>\n",
    "\n",
    "RDD finale viene convertito in json per permettere a elastic search di indicizzare i dati nel database\n",
    "E' importante ricordare di aggiungere un timestamp alla struttura per permettere la futura gestione e visualizzazione su Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ElasticSearch\n",
    "\n",
    "Elasticsearch è un server di ricerca con supporto ad architetture distribuite. Utilizza un'interfaccia RESTful, mentre le informazioni sono gestite come documenti JSON.\n",
    "\n",
    "Indici e documenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setting su Python\n",
    "\n",
    "elastic_host=\"localhost\"\n",
    "elastic_index=\"users\"\n",
    "elastic_document=\"_doc\"\n",
    "\n",
    "es_write_conf = {\n",
    "\"es.nodes\" : elastic_host,\n",
    "\"es.port\" : '9200',\n",
    "\"es.resource\" : '%s/%s' % (elastic_index,elastic_document),\n",
    "\"es.input.json\" : \"yes\"\n",
    "}\n",
    "\n",
    "conf = SparkConf(loadDefaults=False)\n",
    "conf.set(\"es.index.auto.create\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Creazione indice\n",
    "Tutti i json creati in spark convergono verso un unico indice chiamato \"users\".\n",
    "\n",
    "elastic = Elasticsearch(hosts=[elastic_host])\n",
    "\n",
    "response = elastic.indices.create(\n",
    "    index=elastic_index,\n",
    "    body=mapping,\n",
    "    ignore=400 # ignore 400 already exists code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Salvataggio su ES\n",
    "\n",
    "final_rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=es_write_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Verifica se sono stati creati i documenti\n",
    "\n",
    "Comando query\n",
    "\n",
    "E' possibile cancellare l'index attraverso il seguente comando\n",
    "\n",
    "http://localhost:9200/[index_here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KIbana\n",
    "\n",
    "Kibana è una dashboard di visualizzazione dati open source per Elasticsearch. <br />\n",
    "Fornisce funzionalità di visualizzazione in cima al contenuto indicizzato su un cluster Elasticsearch\n",
    "La dashboard è user-friendly e personalizzabile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Creazione indice\n",
    "\n",
    "Prima di poter iniziare e visualizzare i dati è necessario creare e definire un \"index pattern\"\n",
    "\n",
    "foto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Selezione del timestamp field\n",
    "Per poter tracciare i dati indicizzati bisogna specificare quel campo timestamp inserito nel file json creato su spark.\n",
    "\n",
    "foto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DEMO LIVE \n",
    "Passiamo alla demo live, ma prima,\n",
    "un cattivo uso di \"SpeechAnalyzer\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](spark.jpg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
