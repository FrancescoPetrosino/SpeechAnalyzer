# -*- coding: utf-8 -*-
"""
Created on Thu Jul 16 13:51:24 2020

@author: Aldo
"""


import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
from pathlib import Path



import joblib



'''
data = pd.read_csv("abcnews-date-text.csv",error_bad_lines=False,usecols =["headline_text"])
#print(data.head())

data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)
data = data.drop_duplicates('headline_text')

punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)
desc = data['headline_text'].values
'''


documents = []
path = Path("./files/")
for x in path.iterdir():
    data = ""
    with open(x, 'r') as myfile:
       data = myfile.read()
    documents.append(data)
'''

documents = []
with open("merged.txt", 'r') as myfile:
       data = myfile.read()    
documents.append(data)       
'''



stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'[a-zA-Z\']+')

punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_words = text.ENGLISH_STOP_WORDS.union(punc)

def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)
X3 = vectorizer3.fit_transform(documents)
words = vectorizer3.get_feature_names()
print(len(words))

'''
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
    kmeans.fit(X3)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.savefig('elbow.png')
plt.show()

#print(words[250:300])
'''

kmeans = KMeans(n_clusters = 8, n_init = 20, n_jobs = 3) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)
kmeans.fit(X3)
# We look at 3 the clusters generated by k-means.
common_words = kmeans.cluster_centers_.argsort()[:,-1:-35:-1]
for num, centroid in enumerate(common_words):
    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))


filename = 'k_model.sav'
joblib.dump(kmeans, filename)

'''
Y = vectorizer3.transform(["in order to use the software you have to open a browser"])
prediction = kmeans.predict(Y)
print(prediction)
'''
